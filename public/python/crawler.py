import asyncio
import os
import argparse
import logging
from typing import List, Optional, Dict, Any
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
import re
import traceback
from pathlib import Path
import aiofiles

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("crawler.log", encoding='utf-8')  # æ·»åŠ ç¼–ç å‚æ•°
    ]
)
logger = logging.getLogger(__name__)


async def extract_links(url: str, retry_count: int) -> List[str]:
    """ä½¿ç”¨Crawl4AIæå–é“¾æ¥
    
    Args:
        url: è¦æå–é“¾æ¥çš„URL
        retry_count: é‡è¯•æ¬¡æ•°
        
    Returns:
        æå–åˆ°çš„é“¾æ¥åˆ—è¡¨
    """
    for attempt in range(retry_count):
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url)
                if not result.success:
                    logger.warning(f"Failed to extract links from {url}")
                    return []

                # æå–å†…éƒ¨é“¾æ¥
                links = result.links.get("internal", [])

                # å°†linkså»é‡, å¹¶å¤„ç†å¯èƒ½çš„å­—å…¸ç±»å‹
                unique_links = list(
                    set([link["href"] if isinstance(link, dict) else link for link in links])
                )
                logger.info(f"Extracted {len(unique_links)} links from {url}")
                return unique_links
        except Exception as e:
            logger.error(f"Attempt {attempt + 1} failed for {url}: {str(e)}")
            if attempt == retry_count - 1:
                logger.error(f"All attempts failed for {url}")
                return []
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return re.sub(r'[\\/*?:"<>|]', "", filename)


async def save_markdown(mdContent, fullUrl, output_dir):
    """å°†ç»“æœä¿å­˜ä¸ºMarkdownæ–‡ä»¶
    Args:
        mdContent: Markdownå†…å®¹
        fullUrl: å®Œæ•´çš„URL
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    print(f"ğŸ”„ Processing: {fullUrl}")

    if not mdContent:
        print("âŒ Empty content, skipping")
        return

    # è§£æURLï¼Œæå–è·¯å¾„éƒ¨åˆ†ï¼Œå¿½ç•¥æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
    parsed_url = urlparse(fullUrl)
    url_path = parsed_url.path.strip("/")  # å»æ‰é¦–å°¾çš„æ–œæ 
    path_parts = url_path.split("/")  # åˆ†å‰²è·¯å¾„

    # æ„å»ºå­ç›®å½•è·¯å¾„
    subdir = Path(output_dir)
    for part in path_parts[:-1]:  # é™¤äº†æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ–‡ä»¶å
        if part:  # è·³è¿‡ç©ºçš„éƒ¨åˆ†ï¼ˆä¾‹å¦‚åŒæ–œæ æƒ…å†µï¼‰
            subdir = subdir / part
            subdir.mkdir(exist_ok=True)  # åˆ›å»ºå­ç›®å½•

    # æ„å»ºæ–‡ä»¶å
    filename = path_parts[-1] + ".md"
    filepath = subdir / filename

    # å¤„ç†æ–‡ä»¶åå†²çª
    if filepath.exists():
        return

    # å¼‚æ­¥å†™å…¥æ–‡ä»¶
    try:
        async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
            await f.write(mdContent)
        print(f"âœ… Saved: {filepath}")
    except Exception as e:
        print(f"âŒ Error saving {filepath}: {e}")


async def _handle_crawl_result(task, url, output_dir):
    try:
        res = await task
        await save_markdown(res.markdown, url, output_dir)
    except Exception as e:
        print(f"Error processing {url}: {e}")
        print("Traceback:")
        print(traceback.format_exc())


async def crawl_concurrently(urls: List[str], args: argparse.Namespace) -> None:
    """å¹¶å‘çˆ¬å–å¤šä¸ªURL
    
    Args:
        urls: è¦çˆ¬å–çš„URLåˆ—è¡¨
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    semaphore = asyncio.Semaphore(args.max_concurrent)
    
    async def limited_crawl(url: str) -> Optional[Dict[str, Any]]:
        async with semaphore:
            try:
                async with AsyncWebCrawler(
                    config=BrowserConfig(headless=True, text_mode=True),
                ) as crawler:
                    config = CrawlerRunConfig(
                        word_count_threshold=200,
                        wait_until="networkidle",
                        page_timeout=120000,
                    )
                    logger.info(f"ğŸ”— Crawling: {url}")
                    result = await crawler.arun(
                        url=url,
                        config=config,
                    )
                    return result
            except Exception as e:
                logger.error(f"Error crawling {url}: {str(e)}")
                return None

    tasks = []
    for url in urls:
        task = asyncio.create_task(limited_crawl(url))
        task.add_done_callback(
            lambda t, url=url: asyncio.create_task(
                _handle_crawl_result(t, url, args.output)
            )
        )
        tasks.append(task)
    
    await asyncio.gather(*tasks)


async def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ç½‘é¡µçˆ¬è™«")
    parser.add_argument("--url", type=str, default="https://alist.nn.ci/zh/", 
                       help="ç§å­URLï¼Œé»˜è®¤ä¸ºhttps://alist.nn.ci/zh/")
    parser.add_argument("--output", type=str, default="D:\crawl\output_docs", 
                       help="è¾“å‡ºç›®å½•ï¼ˆå¿…é¡»ä»¥_docsç»“å°¾ï¼‰ï¼Œé»˜è®¤ä¸ºD:\crawl\output_docs")
    parser.add_argument("--max-concurrent", type=int, default=10, help="æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼Œé»˜è®¤10")
    parser.add_argument("--retry-count", type=int, default=3, help="é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3")
    args = parser.parse_args()

    # éªŒè¯è¾“å‡ºç›®å½•
    if not args.output.endswith("_docs"):
        print("âŒ è¾“å‡ºç›®å½•å¿…é¡»ä»¥'_docs'ç»“å°¾")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)

    # æ­¥éª¤1ï¼šæå–ä¾§è¾¹æ é“¾æ¥
    print("ğŸ” Extracting sidebar links...")
    links = await extract_links(args.url, args.retry_count)
    print(f"ğŸ“¥ Found {len(links)} links")

    # æ­¥éª¤2ï¼šå¹¶å‘çˆ¬å–å¹¶ä¿å­˜
    print("ğŸš€ Starting concurrent crawling...")
    await crawl_concurrently(links, args)

    print("ğŸ‰ All done!")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ çˆ¬è™«å·²åœæ­¢")
